import asyncio
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools

def get_first_model_name():
    """è·å–ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹åç§°"""
    try:
        from openai import OpenAI
        client = OpenAI(
            base_url="http://10.180.116.5:6390/v1",
            api_key="dummy"
        )
        models = client.models.list()
        return models.data[0].id
    except Exception as e:
        print(f"è·å–æ¨¡å‹åç§°å¤±è´¥: {e}")
        raise

async def test_tool_calling():
    """æµ‹è¯•å·¥å…·è°ƒç”¨åŠŸèƒ½"""
    
    # 1. è®¾ç½® MCP å®¢æˆ·ç«¯
    client = MultiServerMCPClient({
        "excel": {
            "transport": "streamable_http",
            "url": "http://localhost:8007/mcp",
        }
    })
    
    try:
        # 2. è·å–æ¨¡å‹åç§°å¹¶åˆå§‹åŒ– LLM
        model_name = get_first_model_name()
        llm = ChatOpenAI(
            base_url="http://10.180.116.5:6390/v1",
            api_key="dummy",
            model=model_name,
            temperature=0
        )
        
        # 3. åŠ è½½ MCP å·¥å…·
        async with client.session("excel") as session:
            tools = await load_mcp_tools(session)
            print(f"ğŸ”§ åŠ è½½äº† {len(tools)} ä¸ªå·¥å…·")
            
            # 4. æµ‹è¯•å·¥å…·ç»‘å®š
            llm_with_tools = llm.bind_tools(tools)
            
            # 5. å‘é€æµ‹è¯•æ¶ˆæ¯
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Excelæ•°æ®åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. å¿…é¡»é¦–å…ˆä½¿ç”¨æä¾›çš„å·¥å…·è¯»å–Excelæ–‡ä»¶æ•°æ®
2. åŸºäºè¯»å–çš„æ•°æ®è¿›è¡Œåˆ†æ
3. æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Š

é‡è¦ï¼šä½ å¿…é¡»å…ˆè°ƒç”¨å·¥å…·è·å–æ•°æ®ï¼Œç„¶åå†è¿›è¡Œåˆ†æã€‚ä¸è¦åœ¨æ²¡æœ‰è¯»å–æ•°æ®çš„æƒ…å†µä¸‹ç»™å‡ºç­”æ¡ˆã€‚

å¯ç”¨å·¥å…·ï¼š
""" + "\n".join([f"- {tool.name}: {tool.description}" for tool in tools])
            
            user_query = "è¯»å– 20250703it.xlsx çš„ Sheet1ï¼Œå‰300è¡Œ A åˆ° D åˆ—ï¼Œè¯·åˆ†æç”¨æˆ·ä¸»è¦å…³æ³¨å“ªäº›é—®é¢˜ï¼Œå¹¶ç»™å‡ºä¸€ä»½åˆ†ææŠ¥å‘Šã€‚"
            
            messages = [
                HumanMessage(content=system_prompt),
                HumanMessage(content=user_query)
            ]
            
            print("ğŸ¤– å‘é€è¯·æ±‚ç»™LLM...")
            response = llm_with_tools.invoke(messages)
            
            print(f"ğŸ” å“åº”ç±»å‹: {type(response)}")
            print(f"ğŸ” å“åº”å†…å®¹: {response.content[:200]}...")
            print(f"ğŸ” æ˜¯å¦æœ‰tool_callså±æ€§: {hasattr(response, 'tool_calls')}")
            
            if hasattr(response, 'tool_calls') and response.tool_calls:
                print(f"ğŸ” å·¥å…·è°ƒç”¨æ•°é‡: {len(response.tool_calls)}")
                for i, tool_call in enumerate(response.tool_calls):
                    print(f"ğŸ” å·¥å…·è°ƒç”¨ {i+1}: {tool_call}")
            else:
                print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨")
                
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_tool_calling()) 